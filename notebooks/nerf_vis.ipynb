{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "from pytorch3d.io import load_objs_as_meshes\n",
    "from pytorch3d.structures import Volumes\n",
    "from pytorch3d.transforms import so3_exponential_map\n",
    "from pytorch3d.renderer import (\n",
    "    BlendParams,\n",
    "    EmissionAbsorptionRaymarcher,\n",
    "    FoVPerspectiveCameras,\n",
    "    ImplicitRenderer,\n",
    "    look_at_view_transform,\n",
    "    MeshRasterizer,\n",
    "    MeshRenderer,\n",
    "    MonteCarloRaysampler,\n",
    "    NDCGridRaysampler,\n",
    "    PointLights,\n",
    "    RasterizationSettings,\n",
    "    RayBundle,\n",
    "    ray_bundle_to_ray_points,\n",
    "    SoftPhongShader,\n",
    "    SoftSilhouetteShader,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HarmonicEmbedding(torch.nn.Module):\n",
    "    def __init__(self, n_harmonic_functions=60, omega0=0.1):\n",
    "        \"\"\"\n",
    "        Given an input tensor `x` of shape [minibatch, ... , dim],\n",
    "        the harmonic embedding layer converts each feature\n",
    "        in `x` into a series of harmonic features `embedding`\n",
    "        as follows:\n",
    "            embedding[..., i*dim:(i+1)*dim] = [\n",
    "                sin(x[..., i]),\n",
    "                sin(2*x[..., i]),\n",
    "                sin(4*x[..., i]),\n",
    "                ...\n",
    "                sin(2**self.n_harmonic_functions * x[..., i]),\n",
    "                cos(x[..., i]),\n",
    "                cos(2*x[..., i]),\n",
    "                cos(4*x[..., i]),\n",
    "                ...\n",
    "                cos(2**self.n_harmonic_functions * x[..., i])\n",
    "            ]\n",
    "\n",
    "        Note that `x` is also premultiplied by `omega0` before\n",
    "        evaluting the harmonic functions.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.register_buffer(\n",
    "            'frequencies',\n",
    "            omega0 * (2.0 ** torch.arange(n_harmonic_functions)),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: tensor of shape [..., dim]\n",
    "        Returns:\n",
    "            embedding: a harmonic embedding of `x`\n",
    "                of shape [..., n_harmonic_functions * dim * 2]\n",
    "        \"\"\"\n",
    "        embed = (x[..., None] * self.frequencies).view(*x.shape[:-1], -1)\n",
    "        return torch.cat((embed.sin(), embed.cos()), dim=-1)\n",
    "\n",
    "\n",
    "class NeuralRadianceField(torch.nn.Module):\n",
    "    def __init__(self, n_harmonic_functions=60, n_hidden_neurons=256):\n",
    "        super().__init__()\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            n_harmonic_functions: The number of harmonic functions\n",
    "                used to form the harmonic embedding of each point.\n",
    "            n_hidden_neurons: The number of hidden units in the\n",
    "                fully connected layers of the MLPs of the model.\n",
    "        \"\"\"\n",
    "\n",
    "        # The harmonic embedding layer converts input 3D coordinates\n",
    "        # to a representation that is more suitable for\n",
    "        # processing with a deep neural network.\n",
    "        self.harmonic_embedding = HarmonicEmbedding(n_harmonic_functions)\n",
    "\n",
    "        # The dimension of the harmonic embedding.\n",
    "        embedding_dim = n_harmonic_functions * 2 * 3\n",
    "\n",
    "        # self.mlp is a simple 2-layer multi-layer perceptron\n",
    "        # which converts the input per-point harmonic embeddings\n",
    "        # to a latent representation.\n",
    "        # Not that we use Softplus activations instead of ReLU.\n",
    "        self.mlp = torch.nn.Sequential(\n",
    "            torch.nn.Linear(embedding_dim, n_hidden_neurons),\n",
    "            torch.nn.Softplus(beta=10.0),\n",
    "            torch.nn.Linear(n_hidden_neurons, n_hidden_neurons),\n",
    "            torch.nn.Softplus(beta=10.0),\n",
    "        )\n",
    "\n",
    "        # Given features predicted by self.mlp, self.color_layer\n",
    "        # is responsible for predicting a 3-D per-point vector\n",
    "        # that represents the RGB color of the point.\n",
    "        self.color_layer = torch.nn.Sequential(\n",
    "            torch.nn.Linear(n_hidden_neurons + embedding_dim, n_hidden_neurons),\n",
    "            torch.nn.Softplus(beta=10.0),\n",
    "            torch.nn.Linear(n_hidden_neurons, 3),\n",
    "            torch.nn.Sigmoid(),\n",
    "            # To ensure that the colors correctly range between [0-1],\n",
    "            # the layer is terminated with a sigmoid layer.\n",
    "        )\n",
    "\n",
    "        # The density layer converts the features of self.mlp\n",
    "        # to a 1D density value representing the raw opacity\n",
    "        # of each point.\n",
    "        self.density_layer = torch.nn.Sequential(\n",
    "            torch.nn.Linear(n_hidden_neurons, 1),\n",
    "            torch.nn.Softplus(beta=10.0),\n",
    "            # Sofplus activation ensures that the raw opacity\n",
    "            # is a non-negative number.\n",
    "        )\n",
    "\n",
    "        # We set the bias of the density layer to - 1.5 in order to initialize the opacities of the\n",
    "        # ray points to values close to 0.\n",
    "        # This is a crucial detail for ensuring convergence of the model.\n",
    "        self.density_layer[0].bias.data[0] = -1.5\n",
    "\n",
    "    def _get_densities(self, features):\n",
    "        \"\"\"\n",
    "        This function takes `features` predicted by `self.mlp`\n",
    "        and converts them to `raw_densities` with `self.density_layer`.\n",
    "        `raw_densities` are later mapped to [0-1] range with\n",
    "        1 - inverse exponential of `raw_densities`.\n",
    "        \"\"\"\n",
    "        raw_densities = self.density_layer(features)\n",
    "        return 1 - (-raw_densities).exp()\n",
    "\n",
    "    def _get_colors(self, features, rays_directions):\n",
    "        \"\"\"\n",
    "        This function takes per-point `features` predicted by `self.mlp`\n",
    "        and evaluates the color model in order to attach to each\n",
    "        point a 3D vector of its RGB color.\n",
    "\n",
    "        In order to represent viewpoint dependent effects,\n",
    "        before evaluating `self.color_layer`, `NeuralRadianceField`\n",
    "        concatenates to the `features` a harmonic embedding\n",
    "        of `ray_directions`, which are per-point directions\n",
    "        of point rays expressed as 3D l2-normalized vectors\n",
    "        in world coordinates.\n",
    "        \"\"\"\n",
    "        spatial_size = features.shape[:-1]\n",
    "\n",
    "        # Normalize the ray_directions to unit l2 norm.\n",
    "        rays_directions_normed = torch.nn.functional.normalize(\n",
    "            rays_directions, dim=-1\n",
    "        )\n",
    "\n",
    "        # Obtain the harmonic embedding of the normalized ray directions.\n",
    "        rays_embedding = self.harmonic_embedding(\n",
    "            rays_directions_normed\n",
    "        )\n",
    "\n",
    "        # Expand the ray directions tensor so that its spatial size\n",
    "        # is equal to the size of features.\n",
    "        rays_embedding_expand = rays_embedding[..., None, :].expand(\n",
    "            *spatial_size, rays_embedding.shape[-1]\n",
    "        )\n",
    "\n",
    "        # Concatenate ray direction embeddings with\n",
    "        # features and evaluate the color model.\n",
    "        color_layer_input = torch.cat(\n",
    "            (features, rays_embedding_expand),\n",
    "            dim=-1)\n",
    "        return self.color_layer(color_layer_input)\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        ray_bundle,\n",
    "        path=None,\n",
    "        **kwargs,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        The forward function accepts the parametrizations of\n",
    "        3D points sampled along projection rays. The forward\n",
    "        pass is responsible for attaching a 3D vector\n",
    "        and a 1D scalar representing the point's\n",
    "        RGB color and opacity respectively.\n",
    "\n",
    "        Args:\n",
    "            ray_bundle: A RayBundle object containing the following variables:\n",
    "                origins: A tensor of shape `(minibatch, ..., 3)` denoting the\n",
    "                    origins of the sampling rays in world coords.\n",
    "                directions: A tensor of shape `(minibatch, ..., 3)`\n",
    "                    containing the direction vectors of sampling rays in world coords.\n",
    "                lengths: A tensor of shape `(minibatch, ..., num_points_per_ray)`\n",
    "                    containing the lengths at which the rays are sampled.\n",
    "\n",
    "        Returns:\n",
    "            rays_densities: A tensor of shape `(minibatch, ..., num_points_per_ray, 1)`\n",
    "                denoting the opacitiy of each ray point.\n",
    "            rays_colors: A tensor of shape `(minibatch, ..., num_points_per_ray, 3)`\n",
    "                denoting the color of each ray point.\n",
    "        \"\"\"\n",
    "        # We first convert the ray parametrizations to world\n",
    "        # coordinates with `ray_bundle_to_ray_points`.\n",
    "        rays_points_world = ray_bundle_to_ray_points(ray_bundle)\n",
    "        # rays_points_world.shape = [minibatch x ... x 3]\n",
    "\n",
    "        # For each 3D world coordinate, we obtain its harmonic embedding.\n",
    "        embeds = self.harmonic_embedding(\n",
    "            rays_points_world\n",
    "        )\n",
    "        # embeds.shape = [minibatch x ... x self.n_harmonic_functions*6]\n",
    "\n",
    "        # self.mlp maps each harmonic embedding to a latent feature space.\n",
    "        features = self.mlp(embeds)\n",
    "        # features.shape = [minibatch x ... x n_hidden_neurons]\n",
    "\n",
    "        # Finally, given the per-point features,\n",
    "        # execute the density and color branches.\n",
    "\n",
    "        rays_densities = self._get_densities(features)\n",
    "        # rays_densities.shape = [minibatch x ... x 1]\n",
    "\n",
    "        rays_colors = self._get_colors(features, ray_bundle.directions)\n",
    "        # rays_colors.shape = [minibatch x ... x 3]\n",
    "        \n",
    "        if path==None:\n",
    "            return rays_densities, rays_colors\n",
    "        else:\n",
    "            save_io_nerf(rays_points_world, rays_densities, rays_colors)\n",
    "        \n",
    "        return rays_densities, rays_colors\n",
    "    \n",
    "    def save_io_nerf(self, rays_points_world=None, rays_densities=None, rays_colors=None, path='./data/'):\n",
    "        torch.save(rays_points_world, path+'nn_inputs_per_sample')\n",
    "        torch.save(rays_densities, path+'nn_inputs_bla_per_sample')\n",
    "        torch.save(rays_colors, path+'nn_inputs_blubb_per_sample')\n",
    "        \n",
    "        \n",
    "        \n",
    "\n",
    "\n",
    "    # TYXE comment: this was previously a method of the NRF class, I'm now passing a PytorchBNN instance as the net argument\n",
    "    def batched_forward(\n",
    "        self,\n",
    "        ray_bundle,\n",
    "        n_batches,\n",
    "        path,\n",
    "        **kwargs,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        This function is used to allow for memory efficient processing\n",
    "        of input rays. The input rays are first split to `n_batches`\n",
    "        chunks and passed through the `self.forward` function one at a time\n",
    "        in a for loop. Combined with disabling Pytorch gradient caching\n",
    "        (`torch.no_grad()`), this allows for rendering large batches\n",
    "        of rays that do not all fit into GPU memory in a single forward pass.\n",
    "        In our case, batched_forward is used to export a fully-sized render\n",
    "        of the radiance field for visualisation purposes.\n",
    "\n",
    "        Args:\n",
    "            ray_bundle: A RayBundle object containing the following variables:\n",
    "                origins: A tensor of shape `(minibatch, ..., 3)` denoting the\n",
    "                    origins of the sampling rays in world coords.\n",
    "                directions: A tensor of shape `(minibatch, ..., 3)`\n",
    "                    containing the direction vectors of sampling rays in world coords.\n",
    "                lengths: A tensor of shape `(minibatch, ..., num_points_per_ray)`\n",
    "                    containing the lengths at which the rays are sampled.\n",
    "            n_batches: Specifies the number of batches the input rays are split into.\n",
    "                The larger the number of batches, the smaller the memory footprint\n",
    "                and the lower the processing speed.\n",
    "\n",
    "        Returns:\n",
    "            rays_densities: A tensor of shape `(minibatch, ..., num_points_per_ray, 1)`\n",
    "                denoting the opacitiy of each ray point.\n",
    "            rays_colors: A tensor of shape `(minibatch, ..., num_points_per_ray, 3)`\n",
    "                denoting the color of each ray point.\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        # Parse out shapes needed for tensor reshaping in this function.\n",
    "        n_pts_per_ray = ray_bundle.lengths.shape[-1]\n",
    "        spatial_size = [*ray_bundle.origins.shape[:-1], n_pts_per_ray]\n",
    "\n",
    "        # Split the rays to `n_batches` batches.\n",
    "        tot_samples = ray_bundle.origins.shape[:-1].numel()\n",
    "        batches = torch.chunk(torch.arange(tot_samples), n_batches)\n",
    "\n",
    "        # For each batch, execute the standard forward pass.\n",
    "        batch_outputs = [\n",
    "            self(\n",
    "                RayBundle(\n",
    "                    origins=ray_bundle.origins.view(-1, 3)[batch_idx],\n",
    "                    directions=ray_bundle.directions.view(-1, 3)[batch_idx],\n",
    "                    lengths=ray_bundle.lengths.view(-1, n_pts_per_ray)[batch_idx],\n",
    "                    xys=None,\n",
    "                    path=path\n",
    "                )\n",
    "            ) for batch_idx in batches\n",
    "        ]\n",
    "\n",
    "        # Concatenate the per-batch rays_densities and rays_colors\n",
    "        # and reshape according to the sizes of the inputs.\n",
    "        rays_densities, rays_colors = [\n",
    "            torch.cat(\n",
    "                [batch_output[output_i] for batch_output in batch_outputs], dim=0\n",
    "            ).view(*spatial_size, -1) for output_i in (0, 1)\n",
    "        ]\n",
    "        return rays_densities, rays_colors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_rotating_nerf(neural_radiance_field, target_cameras, renderer_grid, device, n_frames=50, num_forward=1, background_path=None):\n",
    "    logRs = torch.zeros(n_frames, 3, device=device)\n",
    "    logRs[:, 1] = torch.linspace(-3.14, 3.14, n_frames, device=device)\n",
    "    Rs = so3_exponential_map(logRs)\n",
    "    Ts = torch.zeros(n_frames, 3, device=device)\n",
    "    Ts[:, 2] = 2.7\n",
    "    frames = []\n",
    "    uncertainties = []\n",
    "    print('Rendering rotating NeRF ...')\n",
    "    for R, T in zip(tqdm(Rs), Ts):\n",
    "        camera = FoVPerspectiveCameras(\n",
    "            R=R[None],\n",
    "            T=T[None],\n",
    "            znear=target_cameras.znear[0],\n",
    "            zfar=target_cameras.zfar[0],\n",
    "            aspect_ratio=target_cameras.aspect_ratio[0],\n",
    "            fov=target_cameras.fov[0],\n",
    "            device=device,\n",
    "        )\n",
    "        # Note that we again render with `NDCGridSampler`\n",
    "        # and the batched_forward function of neural_radiance_field.\n",
    "        frame_samples = torch.stack([renderer_grid(\n",
    "                cameras=camera,\n",
    "                volumetric_function=partial(batched_forward, net=neural_radiance_field, path=background_path)\n",
    "            )[0][..., :3] for _ in range(num_forward)])\n",
    "        frames.append(frame_samples.mean(0))\n",
    "        uncertainties.append(frame_samples.var(0).sum(-1).sqrt() if num_forward > 1 else torch.zeros_like(frame_samples[0, ..., 0]))\n",
    "    return torch.cat(frames), torch.cat(uncertainties)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "neural_radiance_field_net = NeuralRadianceField()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sd = torch.load(\"nerf_ml_params_missing.pt\", map_location=\"cpu\")\n",
    "sd[\"harmonic_embedding.frequencies\"] = neural_radiance_field_net.harmonic_embedding.frequencies\n",
    "neural_radiance_field_net.load_state_dict(sd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
